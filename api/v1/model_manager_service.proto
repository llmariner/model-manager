syntax = "proto3";

package llmariner.models.server.v1;

import "google/api/annotations.proto";

option go_package = "github.com/llmariner/model-manager/api/v1";

// The API specification fllows OpenAPI API specification (https://platform.openai.com/docs/api-reference/models).

enum ModelFormat {
  MODEL_FORMAT_UNSPECIFIED = 0;
  MODEL_FORMAT_GGUF = 1;
  MODEL_FORMAT_HUGGING_FACE = 2;
  // Model format for Nvidia Triton Inference Server. This model files include the tokenizer configuration
  // of the original model, compiled model files for TensorRT-LLM backend, and configuration files for
  // Triton Inference Server.
  MODEL_FORMAT_NVIDIA_TRITON = 3;
  MODEL_FORMAT_OLLAMA = 4;
}

// ModelFormats is a list of model formats. Used to store marshalled data into a store.
message ModelFormats {
  repeated ModelFormat formats = 1;
}

enum ModelLoadingStatus {
  MODEL_LOADING_STATUS_UNSPECIFIED = 0;
  // Intial status when the model creation is requested.
  MODEL_LOADING_STATUS_REQUESTED = 1;
  // Loading status when the model is being loaded.
  MODEL_LOADING_STATUS_LOADING = 2;
  // Succeeded status when the model loading is succeeded.
  MODEL_LOADING_STATUS_SUCCEEDED = 3;
  // Failed status when the model loading is failed.
  MODEL_LOADING_STATUS_FAILED = 4;
}

enum SourceRepository {
  SOURCE_REPOSITORY_UNSPECIFIED = 0;
  SOURCE_REPOSITORY_OBJECT_STORE = 1;
  SOURCE_REPOSITORY_HUGGING_FACE = 2;
  SOURCE_REPOSITORY_OLLAMA = 3;
  SOURCE_REPOSITORY_FINE_TUNING = 4;
}

enum ActivationStatus {
  ACTIVATION_STATUS_UNSPECIFIED = 0;
  ACTIVATION_STATUS_ACTIVE = 1;
  ACTIVATION_STATUS_INACTIVE = 2;
}

message ModelConfig {
  message RuntimeConfig {
    message Resources {
      int32 gpu = 1;
    }

    // resources is the resources required to run the model.
    Resources resources = 1;

    // replicas is the number of replicas to run the model.
    int32 replicas = 2;

    // extra_args is a list of extra arguments to pass to the model runtime.
    repeated string extra_args = 3;
  }

  message ClusterAllocationPolicy {
    // allowed_cluster_ids is a list of cluster IDs where the model can be allocated.
    // If this field is empty, the model can be allocated to any cluster.
    repeated string allowed_cluster_ids = 1;
  }

  // runtime_config is the runtime configuration of the model.
  RuntimeConfig runtime_config = 1;

  // cluster_allocation_policy defines the policy to allocate the model to clusters.
  ClusterAllocationPolicy cluster_allocation_policy = 2;
}

message Model {
  string id = 1;
  int64 created = 2;
  string object = 3;
  string owned_by = 4;

  // loading_status is set when the model is being loaded.
  // This is not in the Open AI API specification.
  ModelLoadingStatus loading_status = 5;
  // source_repository is the source repository of the model. Set for base models created from the CreateModel API request.
  // This is not in the Open AI API specification.
  SourceRepository source_repository = 6;

  // loading_failure_reason is set when the model loading is failed to show the failure reason.
  // This is not in the Open AI API specification.
  string loading_failure_reason = 7;

  // formats is the supported formats of the model.
  // This is not in the Open AI API specification.
  repeated ModelFormat formats = 8;

  // This is not in the Open AI API specification.
  bool is_base_model = 9;

  // base_model_id is the ID of the base model. Only set if the model is a fine-tuned model.
  // This is not in the Open AI API specification.
  string base_model_id = 10;

  ActivationStatus activation_status = 11;

  ModelConfig config = 12;
}

message CreateModelRequest {
  string id = 1;
  SourceRepository source_repository = 2;

  // is_fine_tuned_model is a flag to indicate if the model is a fine-tuned model.
  bool is_fine_tuned_model = 3;
  // base_model_id is the ID of the base model. Only set if the model is a fine-tuned model.
  string base_model_id = 4;
  // suffix is the suffix of the model. Only set if the model is a fine-tuned model.
  string suffix = 5;
  string model_file_location = 6;

  ModelConfig config = 7;

  // is_project_scoped is true if the model has a project scope. Only meaningful for
  // base models.
  bool is_project_scoped = 8;
}

message ListModelsRequest {
  // include_loading_models is a flag to include loading models in the response.
  //
  // This is not in the Open AI API specification.
  bool include_loading_models = 1;

  // after is the identifier for the last model from the previous pagination request.
  // This is not in the Open AI API specification.
  string after = 2;
  // limit is the number of non-base models to retrieve. Defaults to 100.
  // This is not in the Open AI API specification.
  int32 limit = 3;
}

message ListModelsResponse {
  string object = 1;
  repeated Model data = 2;

  // This is not in the Open AI API specification.
  bool has_more = 3;

  // total_items is the total number of non-base models. This is not defined in the
  // OpenAI API spec, but we include here for better UX in the frontend.
  int32 total_items = 4;
}

message GetModelRequest {
  string id = 1;

  // include_loading_model is a flag to include an loading model in the response.
  //
  // This is not in the Open AI API specification.
  bool include_loading_model = 2;
}

message DeleteModelRequest {
  string id = 1;
}

message DeleteModelResponse {
  string id = 1;
  string object = 2;
  bool deleted = 3;
}

message ActivateModelRequest {
  string id = 1;
}

message ActivateModelResponse {
}

message DeactivateModelRequest {
  string id = 1;
}

message DeactivateModelResponse {
}

service ModelsService {

  rpc ListModels(ListModelsRequest) returns (ListModelsResponse) {
    option (google.api.http) = {
      get: "/v1/models"
    };
  }

  rpc GetModel(GetModelRequest) returns (Model) {
    option (google.api.http) = {
      // Note: when '=**' is set, 'id' matchs the whole path including '/'.
      // This is required when the model ID might contain '/' characters.
      // (This happens only for a model that is being loaded. Once it is loaded,
      // the converted ID name does not containe '/').
      get: "/v1/models/{id==**}"
    };
  }

  rpc DeleteModel(DeleteModelRequest) returns (DeleteModelResponse) {
    option (google.api.http) = {
      // Note: when '=**' is set, 'id' matchs the whole path including '/'.
      // This is required when the model ID might contain '/' characters.
      // (This happens only for a model that is being loaded. Once it is loaded,
      // the converted ID name does not containe '/').
      delete: "/v1/models/{id=**}"
    };
  }

  // The following API endpoints are not part of the OpenAPI API specification.

  // CreateModel creates a new model. The model becomes available once
  // its model file is loaded to an object store.
  rpc CreateModel(CreateModelRequest) returns (Model) {
    option (google.api.http) = {
      post: "/v1/models"
      body: "*"
    };
  }

  // TODO(kenji): Add UpdateModel(Config).

  rpc ActivateModel(ActivateModelRequest) returns (ActivateModelResponse) {
    option (google.api.http) = {
      post: "/v1/models/{id}:activate"
    };
  }

  rpc DeactivateModel(DeactivateModelRequest) returns (DeactivateModelResponse) {
    option (google.api.http) = {
      post: "/v1/models/{id}:deactivate"
    };
  }
}

message StorageConfig {
  string path_prefix = 1;
}

message CreateStorageConfigRequest {
  string path_prefix = 1;
}

message GetStorageConfigRequest {
}

enum AdapterType {
  ADAPTER_TYPE_UNSPECIFIED = 0;
  ADAPTER_TYPE_LORA = 1;
  ADAPTER_TYPE_QLORA = 2;
}

enum QuantizationType {
  QUANTIZATION_TYPE_UNSPECIFIED = 0;
  QUANTIZATION_TYPE_GGUF = 1;
  QUANTIZATION_TYPE_AWQ = 2;
}

message RegisterModelRequest {
  // id is the model name.
  string id = 7;
  string base_model = 1;
  string suffix = 2;
  string organization_id = 3;
  string project_id = 4;
  AdapterType adapter = 5;
  QuantizationType quantization = 6;
  string path = 8;

  // Next ID: 9;
}

message RegisterModelResponse {
  string id = 1;
  // path is the path where the model is stored.
  string path = 2;
}

message PublishModelRequest {
  string id = 1;
}

message PublishModelResponse {
}

// GetModelPathRequest is deprecated, use GetModelAttributesRequest instead.
message GetModelPathRequest {
  string id = 1;
}

// GetModelPathResponse is deprecated, use GetModelAttributesResponse instead.
message GetModelPathResponse {
  string path = 1;
}

message ModelAttributes {
  string path = 1;
  string base_model = 2;
  AdapterType adapter = 3;
  QuantizationType quantization = 4;
}

message GetModelAttributesRequest {
  string id = 1;
}

message CreateBaseModelRequest {
  string id = 1;
  string path = 2;

  // formats is the supported formats of the base model. If not set, the format is considered as GGUF
  // for backward compatibility.
  repeated ModelFormat formats = 4;

  // gguf_model_path is the path of the GGUF model. Set if the model suppors the GGUF formta.
  string gguf_model_path = 3;

  SourceRepository source_repository = 5;

  // Next ID: 6
}

message BaseModel {
  string id = 1;
  int64 created = 2;
  string object = 3;
}

message GetBaseModelPathRequest {
  string id = 1;
}

message GetBaseModelPathResponse {
  repeated ModelFormat formats = 3;

  string path = 1;

  // gguf_model_path is the path of the GGUF model. Set if the model suppors the GGUF formta.
  string gguf_model_path = 2;

  // Next ID: 4
}

message CreateHFModelRepoRequest {
  string name = 1;
  string project_id = 2;
}

message HFModelRepo {
  string name = 1;
}

message GetHFModelRepoRequest {
  string name = 1;
  string project_id = 2;
}

message AcquireUnloadedBaseModelRequest {
}

message AcquireUnloadedBaseModelResponse {
  // base_model_id is ID of the acquired base model. Empty if there is no unloaded base model.
  string base_model_id = 1;
  SourceRepository source_repository = 2;
  string project_id = 3;
}

message UpdateBaseModelLoadingStatusRequest {
  string id = 1;

  message Success {
  }

  message Failure {
    string reason = 1;
  }

  oneof loading_result {
    Success success = 2;
    Failure failure = 3;
  }
}

message UpdateBaseModelLoadingStatusResponse {
}

message AcquireUnloadedModelRequest {
}

message AcquireUnloadedModelResponse {
  string model_id = 1;
  bool is_base_model = 2;

  SourceRepository source_repository = 3;
  string model_file_location = 4;

  string dest_path = 5;
}

message UpdateModelLoadingStatusRequest {
  string id = 1;
  bool is_base_model = 2;

  message Success {
  }

  message Failure {
    string reason = 1;
  }

  oneof loading_result {
    Success success = 3;
    Failure failure = 4;
  }
}

message UpdateModelLoadingStatusResponse {
}

service ModelsWorkerService {
  // CreateStorageConfig creates a new storage config. Used by model-manager-loader.
  rpc CreateStorageConfig(CreateStorageConfigRequest) returns (StorageConfig) {
  }

  // GetStorageConfig gets a storage config. Used by model-manager-loader.
  rpc GetStorageConfig(GetStorageConfigRequest) returns (StorageConfig) {
  }

  // GetModel gets a model. Used by inference-manager-engine.
  rpc GetModel(GetModelRequest) returns (Model) {
  }

  // ListModels lists all models. Used by inference-manager-engine.
  // This RPC does not support pagination.
  rpc ListModels(ListModelsRequest) returns (ListModelsResponse) {
  }

  // RegisterModel registers a new fine-tuned model. Used by job-manager-dispatcher and model-manager-loader.
  // The model is not published until PublishModel is called.
  rpc RegisterModel(RegisterModelRequest) returns (RegisterModelResponse) {
  }

  // PublishModel publishes the fine-tuned model. Used by job-manager-dispatcher and model-manager-loader.
  rpc PublishModel(PublishModelRequest) returns (PublishModelResponse) {
  }

  // GetModelPath returns the path of the model. Used by inference-manager-engine and model-manager-loader.
  rpc GetModelPath(GetModelPathRequest) returns (GetModelPathResponse) {
  }

  // GetModelAttributes returns the attributes of the model. Used by inference-manager-engine.
  rpc GetModelAttributes(GetModelAttributesRequest) returns (ModelAttributes) {
  }

  // CreateBaseModel creates a new base model. Used by model-manager-loader.
  rpc CreateBaseModel(CreateBaseModelRequest) returns (BaseModel) {
  }

  // GetBaseModelPath returns the path of the base model. Used by job-manager-dispatcher,
  // inference-manager-engine, and model-manager-loader.
  rpc GetBaseModelPath(GetBaseModelPathRequest) returns (GetBaseModelPathResponse) {
  }

  // CreateHFModelRepo creates a HuggingFace model repo.
  rpc CreateHFModelRepo(CreateHFModelRepoRequest) returns (HFModelRepo) {
  }

  // GetHFModelRepo returns the HuggingFace model repo that has been downloaded. Used by model-manager-loader.
  rpc GetHFModelRepo(GetHFModelRepoRequest) returns (HFModelRepo) {
  }

  // AcquireUnloadedBaseModel checks if there is any unloaded base model. If exists, update the loading status to LOADED,
  // and return it. Used by model-manager-loader.
  rpc AcquireUnloadedBaseModel(AcquireUnloadedBaseModelRequest) returns (AcquireUnloadedBaseModelResponse) {
  }

  // AcquireUnloadedModel checks if there is any unloaded model. If exists, update the loading status to LOADED,
  // and return it. Used by model-manager-loader.
  rpc AcquireUnloadedModel(AcquireUnloadedModelRequest) returns (AcquireUnloadedModelResponse) {
  }

  // UpdateBaseModelLoadingStatus updates the loading status. When the loading succeeded, it also
  // updates the base model metadata. Used by model-manager-loader.
  rpc UpdateBaseModelLoadingStatus(UpdateBaseModelLoadingStatusRequest) returns (UpdateBaseModelLoadingStatusResponse) {
  }

  // UpdateModelLoadingStatus updates the loading status. When the loading succeeded, it also
  // updates the model metadata. Used by model-manager-loader.
  rpc UpdateModelLoadingStatus(UpdateModelLoadingStatusRequest) returns (UpdateModelLoadingStatusResponse) {
  }

}
